In this project, we utilized IBM's Granite 3B Instruct model in the quantized GGUF format (granite-3b-2b-instruct-Q4_K_M.gguf). This model is optimized for instruction-following tasks and supports lightweight, efficient inference using llama-cpp-python. It plays a central role in powering various stages of the SDLC automation, such as requirement analysis, code generation, test case creation, bug fixing, and documentation. Its compact size and performance efficiency make it suitable for local deployment without the need for GPU, enabling smoother integration into real-world development pipelines.